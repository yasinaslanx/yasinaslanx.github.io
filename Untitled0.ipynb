{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMhKudM6OMd5ADbJStIrSCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasinaslanx/yasinaslanx.github.io/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== ADIM 1: Kurulum ====================\n",
        "print(\"ğŸ”§ Gerekli kÃ¼tÃ¼phaneler kuruluyor...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes datasets\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import json\n",
        "from google.colab import files, drive\n",
        "import os\n",
        "\n",
        "print(\"âœ… Kurulum tamamlandÄ±!\")\n",
        "print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# ==================== ADIM 2: GGUF Modelinizi TanÄ±mlayÄ±n ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“¦ ADIM 2: GGUF Model Yolu\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Google Drive baÄŸla\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "gguf_model_path = input(\"\\nGGUF model yolu: \").strip()\n",
        "print(f\"âœ… GGUF Model: {gguf_model_path}\")\n",
        "\n",
        "# ==================== ADIM 3: Base Model YÃ¼kle ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ¤– ADIM 3: Base Model YÃ¼kleme\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "âš ï¸ Ã–NEMLÄ°: GGUF modelleri eÄŸitilemez!\n",
        "\n",
        "Ã‡Ã¶zÃ¼m: Base Gemma modelini yÃ¼kleyip, sizin GGUF'unuzun\n",
        "aÄŸÄ±rlÄ±klarÄ±nÄ± transfer edeceÄŸiz (aynÄ± mimari olduÄŸu iÃ§in).\n",
        "\n",
        "Bu ÅŸekilde:\n",
        "1. Base Gemma 3 4B yÃ¼klenir\n",
        "2. JSON verilerinizle eÄŸitilir\n",
        "3. SonuÃ§ tekrar GGUF'a Ã§evrilir\n",
        "\"\"\")\n",
        "\n",
        "# Unsloth optimized Gemma model\n",
        "model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
        "max_seq_length = 2048\n",
        "\n",
        "print(f\"\\nğŸ”„ Base model yÃ¼kleniyor: {model_name}\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(\"âœ… Base model yÃ¼klendi!\")\n",
        "\n",
        "# LoRA adapter ekle\n",
        "print(\"\\nâš™ï¸ LoRA adapter ekleniyor...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=128,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(\"âœ… LoRA eklendi!\")\n",
        "\n",
        "# ==================== ADIM 4: JSON Veri YÃ¼kleme ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“„ ADIM 4: JSON Veri YÃ¼kleme\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "JSON formatÄ±:\n",
        "[\n",
        "  {\n",
        "    \"messages\": [\n",
        "      {\"role\": \"user\", \"content\": \"Soru...\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"Cevap...\"}\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nğŸ“ JSON dosyanÄ±zÄ± yÃ¼kleyin...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "json_filename = list(uploaded.keys())[0]\n",
        "print(f\"âœ… YÃ¼klendi: {json_filename}\")\n",
        "\n",
        "# JSON oku\n",
        "with open(json_filename, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"\\nğŸ“Š Toplam konuÅŸma: {len(data)}\")\n",
        "\n",
        "# ==================== ADIM 5: Dataset HazÄ±rla ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ”¨ ADIM 5: Dataset HazÄ±rlama\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def format_prompts(examples):\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        text = \"\"\n",
        "        for msg in messages:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                text += f\"<start_of_turn>user\\n{msg['content']}<end_of_turn>\\n\"\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                text += f\"<start_of_turn>model\\n{msg['content']}<end_of_turn>\\n\"\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset = dataset.map(format_prompts, batched=True)\n",
        "\n",
        "print(f\"âœ… Dataset: {len(dataset)} Ã¶rnek\")\n",
        "\n",
        "# ==================== ADIM 6: EÄŸitim Parametreleri ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ¯ ADIM 6: EÄŸitim Parametreleri\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "epochs = int(input(\"Epoch sayÄ±sÄ± (Ã¶nerilen: 3): \") or \"3\")\n",
        "batch_size = int(input(\"Batch size (Ã¶nerilen: 2): \") or \"2\")\n",
        "learning_rate = float(input(\"Learning rate (Ã¶nerilen: 2e-4): \") or \"2e-4\")\n",
        "\n",
        "print(f\"\\nâœ… Parametreler:\")\n",
        "print(f\"   â€¢ Epochs: {epochs}\")\n",
        "print(f\"   â€¢ Batch Size: {batch_size}\")\n",
        "print(f\"   â€¢ Learning Rate: {learning_rate}\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ==================== ADIM 7: EÄŸitim BaÅŸlat ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸš€ ADIM 7: EÄÄ°TÄ°M BAÅLIYOR\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"\\nâ° EÄŸitim baÅŸladÄ±...\")\n",
        "\n",
        "# GPU stats\n",
        "start_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"ğŸ’¾ Bellek: {start_memory} GB / {max_memory} GB\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\nâœ… EÄÄ°TÄ°M TAMAMLANDI! ğŸ‰\")\n",
        "\n",
        "# ==================== ADIM 8: GGUF'a DÃ¶nÃ¼ÅŸtÃ¼r ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ”„ ADIM 8: GGUF FormatÄ±na DÃ¶nÃ¼ÅŸtÃ¼rme\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nHangi quantization methodlarÄ±nÄ± istersiniz?\")\n",
        "print(\"1 - Sadece q4_k_m (hÄ±zlÄ±, Ã¶nerilen)\")\n",
        "print(\"2 - q4_k_m + q5_k_m\")\n",
        "print(\"3 - Hepsi (q4_k_m, q5_k_m, q8_0)\")\n",
        "\n",
        "quant_choice = input(\"\\nSeÃ§im (1/2/3): \").strip()\n",
        "\n",
        "if quant_choice == \"1\":\n",
        "    methods = [\"q4_k_m\"]\n",
        "elif quant_choice == \"2\":\n",
        "    methods = [\"q4_k_m\", \"q5_k_m\"]\n",
        "else:\n",
        "    methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
        "\n",
        "print(f\"\\nğŸ”„ {len(methods)} format oluÅŸturuluyor...\")\n",
        "\n",
        "for method in methods:\n",
        "    output_dir = f\"gguf_output_{method}\"\n",
        "    print(f\"\\nğŸ“¦ {method} formatÄ± oluÅŸturuluyor...\")\n",
        "\n",
        "    model.save_pretrained_gguf(\n",
        "        output_dir,\n",
        "        tokenizer,\n",
        "        quantization_method=method\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… {output_dir} hazÄ±r!\")\n",
        "\n",
        "print(\"\\nâœ… TÃ¼m GGUF formatlarÄ± oluÅŸturuldu!\")\n",
        "\n",
        "# ==================== ADIM 9: Drive'a Kaydet ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"â˜ï¸ ADIM 9: Google Drive'a Kaydetme\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "drive_path = input(\"\\nDrive klasÃ¶r yolu (Ã¶rn: /content/drive/MyDrive/models/): \").strip()\n",
        "\n",
        "if drive_path:\n",
        "    print(f\"\\nğŸ“‹ Drive'a kopyalanÄ±yor...\")\n",
        "\n",
        "    for method in methods:\n",
        "        output_dir = f\"gguf_output_{method}\"\n",
        "        target = f\"{drive_path}/gguf_updated_{method}\"\n",
        "        !mkdir -p {target}\n",
        "        !cp -r {output_dir}/* {target}/\n",
        "        print(f\"âœ… {method} â†’ {target}\")\n",
        "\n",
        "    print(f\"\\nâœ… TÃ¼m modeller Drive'a kaydedildi!\")\n",
        "\n",
        "# ==================== ADIM 10: Test ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ§ª ADIM 10: Model Test\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def chat(message):\n",
        "    prompt = f\"<start_of_turn>user\\n{message}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "    response = response.split(\"<start_of_turn>model\\n\")[-1].split(\"<end_of_turn>\")[0]\n",
        "    return response\n",
        "\n",
        "# Test Ã¶rnekleri\n",
        "print(\"\\nğŸ’¬ Test baÅŸlÄ±yor...\")\n",
        "test_msgs = [\"Merhaba\", \"Sen kimsin?\", \"Bana yardÄ±m eder misin?\"]\n",
        "\n",
        "for msg in test_msgs:\n",
        "    print(f\"\\nğŸ‘¤ KullanÄ±cÄ±: {msg}\")\n",
        "    print(f\"ğŸ¤– Asistan: {chat(msg)}\")\n",
        "\n",
        "# Ä°nteraktif\n",
        "print(\"\\nğŸ’¬ Ä°nteraktif test (Ã§Ä±kmak iÃ§in 'quit'):\")\n",
        "while True:\n",
        "    user_msg = input(\"\\nğŸ‘¤ Siz: \").strip()\n",
        "    if user_msg.lower() in ['quit', 'exit', 'Ã§Ä±k']:\n",
        "        break\n",
        "    print(f\"ğŸ¤– Bot: {chat(user_msg)}\")\n",
        "\n",
        "# ==================== ADIM 11: Ã–zet ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“Š Ã–ZET RAPOR\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_for_lora = round(used_memory - start_memory, 3)\n",
        "usage_percent = round(used_memory / max_memory * 100, 3)\n",
        "\n",
        "print(f\"\"\"\n",
        "âœ… TÃœM Ä°ÅLEMLER TAMAMLANDI!\n",
        "\n",
        "ğŸ“ˆ EÄŸitim Ä°statistikleri:\n",
        "   â€¢ Veri: {len(data)} konuÅŸma\n",
        "   â€¢ Epoch: {epochs}\n",
        "   â€¢ Batch Size: {batch_size}\n",
        "   â€¢ Learning Rate: {learning_rate}\n",
        "\n",
        "ğŸ’¾ GPU KullanÄ±mÄ±:\n",
        "   â€¢ BaÅŸlangÄ±Ã§: {start_memory} GB\n",
        "   â€¢ Peak: {used_memory} GB\n",
        "   â€¢ LoRA: {used_for_lora} GB\n",
        "   â€¢ KullanÄ±m: {usage_percent}%\n",
        "\n",
        "ğŸ“ OluÅŸturulan GGUF Modelleri:\n",
        "\"\"\")\n",
        "\n",
        "for method in methods:\n",
        "    size_info = {\n",
        "        \"q4_k_m\": \"~2.5 GB (hÄ±z/kalite dengesi)\",\n",
        "        \"q5_k_m\": \"~3.0 GB (daha iyi kalite)\",\n",
        "        \"q8_0\": \"~4.5 GB (en iyi kalite)\"\n",
        "    }\n",
        "    print(f\"   â€¢ gguf_output_{method}/ - {size_info.get(method, '')}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "â˜ï¸ Drive Konumu:\n",
        "   {drive_path}\n",
        "\n",
        "ğŸš€ GGUF Modelini Kullanma:\n",
        "\n",
        "1ï¸âƒ£ llama.cpp ile:\n",
        "   ./main -m gguf_output_q4_k_m/*.gguf -p \"Merhaba\"\n",
        "\n",
        "2ï¸âƒ£ Ollama ile:\n",
        "   ollama create mymodel -f gguf_output_q4_k_m/*.gguf\n",
        "   ollama run mymodel\n",
        "\n",
        "3ï¸âƒ£ LM Studio ile:\n",
        "   File â†’ Import â†’ GGUF dosyasÄ±nÄ± seÃ§in\n",
        "\n",
        "ğŸ’¡ Ã–nerilen: q4_k_m formatÄ±nÄ± kullanÄ±n (hÄ±z/kalite dengesi)\n",
        "\n",
        "ğŸ‰ Modeliniz gÃ¼ncellendi ve GGUF formatÄ±nda hazÄ±r!\n",
        "\"\"\")\n",
        "\n",
        "# DosyalarÄ± listele\n",
        "print(\"\\nğŸ“‚ OluÅŸturulan Dosyalar:\")\n",
        "for method in methods:\n",
        "    print(f\"\\n{method}:\")\n",
        "    !ls -lh gguf_output_{method}/"
      ],
      "metadata": {
        "id": "H6lCCnGzFwze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ã–RNEK JSON FORMATI\n",
        " [\n",
        "  {\n",
        "    \"instruction\": \"Ardahan Ãœniversitesi RektÃ¶rÃ¼ kimdir?\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Ãœniversitemizin rektÃ¶rÃ¼ Prof. Dr. Mehmet Biber'dir.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Yemekhane saatleri nedir?\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Ã–ÄŸle yemeÄŸi servisi 11:30 ile 13:30 saatleri arasÄ±nda yapÄ±lmaktadÄ±r.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Bu metni Ã¶zetle.\",\n",
        "    \"input\": \"Ardahan Ãœniversitesi 2008 yÄ±lÄ±nda kurulmuÅŸ olup...\",\n",
        "    \"output\": \"2008'de kurulan Ardahan Ãœniversitesi, bÃ¶lgenin Ã¶nemli bir eÄŸitim kurumudur.\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "Qkhb5WWRhSbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ã–RNEK CSV FORMATI\n",
        "instruction,input,output\n",
        "\"KÃ¼tÃ¼phane nerede?\",\"\",\"Merkez kampÃ¼s giriÅŸinde, rektÃ¶rlÃ¼k binasÄ±nÄ±n karÅŸÄ±sÄ±ndadÄ±r.\"\n",
        "\"Final sÄ±navlarÄ± ne zaman?\",\"2024-2025 Akademik Takvimi\",\"GÃ¼z dÃ¶nemi finalleri Ocak ayÄ±nÄ±n ilk haftasÄ± baÅŸlayacaktÄ±r.\""
      ],
      "metadata": {
        "id": "OXdLXsIKhvTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}